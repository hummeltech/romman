#!/usr/bin/env python3

## Romman - yet another tool to compare your console ROMs with accuracy-focused datasheets
## Copyright (c) 2021 moonburnt
##
## This program is licensed under Anti-Capitalist Software License.
## For terms and conditions, see attached LICENSE file.

import logging
import romman
from sys import exit
import argparse
from os.path import join

TOOL_NAME = romman.configuration.TOOL_NAME
LAUNCHER_NAME = f"{TOOL_NAME}-cli"
PROGRAM_DIRECTORY = romman.configuration.PROGRAM_DIRECTORY
CACHE_DIRECTORY = romman.configuration.CACHE_DIRECTORY
DEFAULT_DATASHEETS_DIRECTORY = romman.configuration.DEFAULT_DATASHEETS_DIRECTORY

log = logging.getLogger()
log.setLevel(logging.INFO)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter(fmt='[%(asctime)s][%(name)s][%(levelname)s] %(message)s', datefmt='%H:%M:%S'))
log.addHandler(handler)

#argparse shenanigans
ap = argparse.ArgumentParser()
ap.add_argument("items", help="Path to ROM file or directory with ROMs to compare with database. Can be multiple. If directory - will fetch content from all subdirectories", nargs='*', type=str)
ap.add_argument("--datfiles", help="Custom path to data file or directory with data files, used to compare ROMs with instead of default. Can be multiple. Also process subdirectories", nargs='*', type=str)
ap.add_argument("--debug", help="Add debug messages to program's output", action="store_true")
ap.add_argument("--update-datfiles", help="Fetch newest available datfiles from sites of their authors", action="store_true")
args = ap.parse_args()

if args.debug:
    log.setLevel(logging.DEBUG) #Overriding default value from above

if args.update_datfiles:
    log.info("Updating the database (may take some time)")
    romman.dat_updater.datasheets_updater()
    log.info("Successfully updated the database!")

if not args.items:
    print(f"Got no ROMs to verify! For usage info, see {LAUNCHER_NAME} -h")
    exit(0)

if args.datfiles:
    datasheets = args.datfiles
    log.debug(f"Attempting to load {args.datfiles} instead of default datasheets")
else:
    datasheets = [DEFAULT_DATASHEETS_DIRECTORY]

log.info("Loading the database")
data_files = []
for item in datasheets:
    try:
        df = romman.file_processing.get_files(item)
    except Exception as e:
        log.warning(f"Couldnt process datasheets on path {item}: {e}")
        continue
    else:
        data_files.extend(df)

if not data_files:
    log.critical(f"Couldnt find any valid data files! Abort")
    exit(1)

database = []
for item in data_files:
    #this check is meh, but important - applying wrong parser to wrong file worthy hundreds of mbytes may eat all ram
    if item.endswith('.dat'.lower()):
        log.debug(f"Guessing that {item} is standard .dat file and proceeding accordingly")
        parser = romman.data_parsers.dat_file
    elif item.endswith('.xml'.lower()):
        log.debug(f"Guessing that {item} is mame .xml file and proceeding accordingly")
        parser = romman.data_parsers.mame_xml
    else:
        log.warning(f"Couldnt process data file {item}: unknown file format")
        continue

    try:
        data = parser(item)
    except Exception as e:
        log.warning(f"Couldnt process data file {item}: {e}")
    else:
        database.extend(data)

if not database:
    log.critical(f"Couldnt find any valid database entries! Abort")
    exit(1)

log.info("Getting the filelist from provided arguments")
filepaths = []
for item in args.items:
    try:
        f = romman.file_processing.get_files(item)
    except Exception as e:
        log.warning(f"Couldnt get files from {item}: {e}. Skipping")
        continue
    else:
        filepaths.extend(f)

log.info("Calculating hash sums of provided files")
files = []
for item in filepaths:
    try:
        data = romman.file_processing.file_processor(item)
    except Exception as e:
        log.warning(f"Couldnt get hash of {item}: {e}. Skipping")
        continue
    else:
        files.extend(data)

if not files:
    log.critical(f"No valid file entries has been received! Abort")
    exit(1)

log.debug(f"Got following files to process: {files}")
#This will only find the very first match. If file exists in multiple datsheets - the rest will be ignored
matches = 0
for item in files:
    for entry in database:
        try:
            #I should probably bring everything to lowercase while fetching hashes from datasheets
            #thus said - "if it works, it works", even if it may be less resource-efficient
            if item['crc'] == entry['crc'].lower():
                log.info(f"{item['path']} match {entry['name']} in {entry['group']} of {entry['category']}!")
                matches += 1
                break #this only breaks "for entry" cycle - "for item" continues with next file
        except KeyError:
            #this is debug, coz it floods the output on mame xmls otherwise
            #if I will really add ability to use md5/sha1 instead of crc - maybe I should do separate check?
            #e.g additional loop that will remove all database entries without info about selected hashsum type
            log.debug(f"Couldnt compare {item['path']} with {entry['name']}: data entry has no valid hash information. Skipping")
            continue
        except Exception as e:
            log.warning(f"Couldnt compare {item['path']} with {entry['name']}: {e}. Skipping")
            continue

errors = len(filepaths) - len(files)
misses = len(files) - matches

print(f"{TOOL_NAME} has finished its job: got {matches} hits, {misses} misses and {errors} errors")
